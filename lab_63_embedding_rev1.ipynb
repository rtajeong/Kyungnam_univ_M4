{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab_63_embedding_rev1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtajeong/Kyungnam_univ_M4/blob/main/lab_63_embedding_rev1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pa94qBD7EKp"
      },
      "source": [
        "# 단어 임베딩 (words -> continuous vector space 로 projection)\n",
        "- words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
        "- The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
        "- The position of a word in the learned vector space is referred to as its embedding.\n",
        "\n",
        "### Embedding 계층을 사용하여 쉽게 만들 수 있다\n",
        "- 정수 인덱스를 벡터로 매핑하는 딕셔너리 구조 (인덱스 크기, 벡터 크기)\n",
        "- 학습 시키는 데이터에 따라 다른 임베딩이 만들어진다.\n",
        "\n",
        "### IMDB 영화 리뷰 데이터를 사용한 임베딩 예제\n",
        "- IMDB: (internet movie database) the world's most popular and authoritative source for movie, TV and celebrity content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8hRGgn85LtI"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
        "import os, os.path\n",
        "import zipfile\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZeMWh1AX2i"
      },
      "source": [
        "- 10000 개의 단어만 사용하고, 각 문장에서는 뒤에서부터 20 개의 단어만 사용하겠음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tu8xAnn_nGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55de5eb0-0ff9-44de-d5f0-341cf396db83"
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 20\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXuG1WkZeq6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10af55e8-4b52-40c4-c8fd-2a40818b3758"
      },
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,), (25000,), (25000,), (25000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3AbuPD9rNde",
        "outputId": "62e00664-e870-4d88-c1a7-2193937c1c58"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmlO1XkY2tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac671d0-5be7-45a8-c283-55482d44818a"
      },
      "source": [
        "# 각 문장이 몇개의 단어로 구성되어 있는지 확인\r\n",
        "[len(x_train[i]) for i in range(10)]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4w1ON5Z7SvH",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df2a4cf-37b9-458c-8f01-993398f33526"
      },
      "source": [
        "x_train[0:2]   # words tokenized and expressed by (word) numbers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blQtfwq4e3NP"
      },
      "source": [
        "# 마지막 20개의 단어들만 사용한다. -> 20개보다 적으면 똑같은 길이로 만들어 준다. padding position is 'post'\n",
        "x_train_p=preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "x_test_p=preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_WuOAkafd8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8c7acb-3f81-4b6a-8f9f-66ae29092aba"
      },
      "source": [
        "x_train_p[0:2]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
              "         113,  103,   32,   15,   16, 5345,   19,  178,   32],\n",
              "       [  23,    4, 1690,   15,   16,    4, 1355,    5,   28,    6,   52,\n",
              "         154,  462,   33,   89,   78,  285,   16,  145,   95]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVALCBnksmms"
      },
      "source": [
        "- Embedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. 이 때 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스입니다. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimensionality)인 3D 텐서를 리턴합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD5dWA__fqO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ec05d6-5b6d-49ca-dbed-85ee1ebdc5b8"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen)) # input 각 단어에 대해 8-vector 로 임베딩\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiMsP3C96RRg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27c22a4-88d8-4011-90e1-04602c6e08a2"
      },
      "source": [
        "history = model.fit(x_train_p, y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 3ms/step - loss: 0.6864 - acc: 0.5658 - val_loss: 0.6311 - val_acc: 0.6940\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5847 - acc: 0.7400 - val_loss: 0.5365 - val_acc: 0.7280\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4814 - acc: 0.7808 - val_loss: 0.5045 - val_acc: 0.7432\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4241 - acc: 0.8135 - val_loss: 0.4967 - val_acc: 0.7544\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3944 - acc: 0.8288 - val_loss: 0.4949 - val_acc: 0.7546\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3663 - acc: 0.8406 - val_loss: 0.4978 - val_acc: 0.7550\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3441 - acc: 0.8539 - val_loss: 0.5030 - val_acc: 0.7568\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3252 - acc: 0.8651 - val_loss: 0.5129 - val_acc: 0.7514\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3010 - acc: 0.8759 - val_loss: 0.5204 - val_acc: 0.7504\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2909 - acc: 0.8868 - val_loss: 0.5295 - val_acc: 0.7486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYmQslQ_Edk"
      },
      "source": [
        "## 위의 결과는 20개의 단어만 고려한 것임\n",
        "### 성능이 75% 정도 됨\n",
        "- 각 단어를 독립적으로 다루었으며, 문장의 구성 정보를 고려하지 않음\n",
        "- 문장의 구조 정보를 고려하려면 임베딩 층 위에 합성곱이나 순환신경망 층을 추가한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNRUc-dA0i9C"
      },
      "source": [
        "## RNN 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrnA5C8y30oX",
        "outputId": "c6635c6f-3798-4d6a-b782-d9cb31bd2b1a"
      },
      "source": [
        "x_train_p.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGNtU9Bz0idI",
        "outputId": "dd43c775-99ef-432d-c3b0-91e172002c4a"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=8))\n",
        "model.add(SimpleRNN(128))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 8)           80000     \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               17536     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 97,665\n",
            "Trainable params: 97,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqusSXm75Lun",
        "outputId": "0dbbb883-d1ac-477c-f1fb-fc4221f1bf0b"
      },
      "source": [
        "history = model.fit(x_train_p, y_train,\n",
        "                    epochs=20, batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.6651 - acc: 0.5665 - val_loss: 0.5473 - val_acc: 0.7258\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.4764 - acc: 0.7766 - val_loss: 0.5052 - val_acc: 0.7472\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.4308 - acc: 0.8011 - val_loss: 0.4944 - val_acc: 0.7536\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.4081 - acc: 0.8159 - val_loss: 0.5286 - val_acc: 0.7516\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.3879 - acc: 0.8260 - val_loss: 0.5238 - val_acc: 0.7490\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3723 - acc: 0.8365 - val_loss: 0.5035 - val_acc: 0.7562\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3531 - acc: 0.8487 - val_loss: 0.5339 - val_acc: 0.7492\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3269 - acc: 0.8645 - val_loss: 0.5817 - val_acc: 0.7478\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2935 - acc: 0.8781 - val_loss: 0.5792 - val_acc: 0.7422\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2567 - acc: 0.8929 - val_loss: 0.6773 - val_acc: 0.7416\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2329 - acc: 0.9067 - val_loss: 0.6556 - val_acc: 0.7330\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1997 - acc: 0.9222 - val_loss: 0.7013 - val_acc: 0.7212\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1793 - acc: 0.9289 - val_loss: 0.7841 - val_acc: 0.7174\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1614 - acc: 0.9371 - val_loss: 0.7201 - val_acc: 0.7222\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1432 - acc: 0.9478 - val_loss: 0.8249 - val_acc: 0.7260\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1296 - acc: 0.9509 - val_loss: 0.8972 - val_acc: 0.6924\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1113 - acc: 0.9594 - val_loss: 0.8954 - val_acc: 0.7278\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1066 - acc: 0.9599 - val_loss: 0.9689 - val_acc: 0.6892\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.1020 - acc: 0.9636 - val_loss: 0.9451 - val_acc: 0.7128\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.0882 - acc: 0.9683 - val_loss: 0.9486 - val_acc: 0.7030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POhP6d5oEbBw"
      },
      "source": [
        "# 연습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoaeBMEeEec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00e79c6-699b-4890-b8d0-192870906b32"
      },
      "source": [
        "import tensorflow as tf\n",
        "# 문장 토큰화와 단어 토큰화\n",
        "text=[['Hope', 'to', 'see', 'you', 'soon'],['Nice', 'to', 'see', 'you', 'again']]\n",
        "\n",
        "# 각 단어에 대한 정수 인코딩\n",
        "text=[[0, 1, 2, 3, 4],[5, 1, 2, 3, 6]]\n",
        "\n",
        "# 위 데이터가 아래의 임베딩 층의 입력이 된다.\n",
        "embedding_layer = Embedding(7, 2, input_length=5)\n",
        "result = embedding_layer(tf.constant([0, 1, 2, 3, 4, 5, 6]))\n",
        "print(result.numpy())\n",
        "\n",
        "# 7은 단어의 개수. 즉, 단어 집합(vocabulary)의 크기이다.\n",
        "# 2는 임베딩한 후의 벡터의 크기이다.\n",
        "# 5는 각 입력 시퀀스의 길이. 즉, input_length이다. 아래와 같은 형태가 됨.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f4b325fe208>\n",
            "[[-0.04526671 -0.00644044]\n",
            " [-0.03116806 -0.01851275]\n",
            " [-0.03980669  0.01063291]\n",
            " [-0.04599432 -0.02542104]\n",
            " [-0.04872879 -0.01556901]\n",
            " [-0.0287668  -0.04047495]\n",
            " [ 0.03014007 -0.02833869]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08IEz9YryFE",
        "outputId": "565fae89-459f-4d5b-e41c-6fc8f31a9b4a"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(7, 2, input_length=5))\r\n",
        "model.add(Flatten())\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 5, 2)              14        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14\n",
            "Trainable params: 14\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZFOcyZasntE",
        "outputId": "4a196bb9-c059-449e-9811-8b28ec27f40d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['Hope to see you soon',\n",
        "         'Nice to see you again']\n",
        "cv = CountVectorizer()\n",
        "cv.fit_transform(corpus).toarray() , cv.get_feature_names(), cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 1, 0, 1, 1, 1, 1],\n",
              "        [1, 0, 1, 1, 0, 1, 1]]),\n",
              " ['again', 'hope', 'nice', 'see', 'soon', 'to', 'you'],\n",
              " {'again': 0, 'hope': 1, 'nice': 2, 'see': 3, 'soon': 4, 'to': 5, 'you': 6})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPpriV7NEdAv"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "text=[['Hope', 'to', 'see', 'you', 'soon'],\n",
        "      ['Nice', 'to', 'see', 'you', 'again']]\n",
        "\n",
        "model = Word2Vec(text, min_count=1, size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k8gEguswayh",
        "outputId": "dadd432f-1736-4d3d-86b5-770f6c80aaa8"
      },
      "source": [
        "for i in range(len(text)):\n",
        "    print(model[text[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.23644601  0.13282813]\n",
            " [-0.10490499  0.02300035]\n",
            " [ 0.09623462  0.12984623]\n",
            " [ 0.2290124  -0.01340628]\n",
            " [ 0.07774924  0.02938988]]\n",
            "[[-0.00383358 -0.0236154 ]\n",
            " [-0.10490499  0.02300035]\n",
            " [ 0.09623462  0.12984623]\n",
            " [ 0.2290124  -0.01340628]\n",
            " [ 0.24975368  0.22249663]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MSa403gwWum"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}